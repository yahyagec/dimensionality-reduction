import pandas as pd
import numpy as np
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from sklearn.cross_validation import train_test_split as tts
import dcor
from sklearn.cluster import KMeans as km
from scipy import stats
from sklearn.linear_model import Ridge
from sklearn.preprocessing import PolynomialFea1tures
from sklearn.pipeline import Pipeline
from itertools import groupby
import math
import io
from sklearn.random_projection import SparseRandomProjection
from multimethod import multimethod
from sklearn.model_selection import RandomizedSearchCV as rscv

class numerics():
    def __init__(num =  ['int16', 'int32', 'int64', 'float16', 'float32', 'float64'])
        self.num = num
    
class AlgType(Enum):
    xgboost_regressor = XGBRegressor()
    xgboost_classifier = XGBClassifier()
    lightgbm_regressor = LGBMRegressor()
    lightgbm_classifier = LGBMClassifier()
    catboost_regressor = CatBoostRegressor()
    catboost_classifier = CatBoostClassifier()
    
class DimensionalityReduction():
    
    def __init__(self, raw_data, target_name, meta_data=None):
        self.raw_data = raw_data
        assert(isinstance(target_name, str), 'target_name must be string!') 
        self.values = raw_data.drop([target_name], axis=1) 
        self.target = raw_data[target_name]
        self.meta_data = meta_data
        
        
#    def get_data(self, target_name):
#        assert(isinstance(target_name, str), 'target_name must be string!')
#        values = self.raw_data.drop([target_name], axis=1) 
#        target = self.raw_data[target_name]
#        return values, target

  
    def encoder(self, values=self.values, meta_data=self.meta_data):
        categorics = np.array([y for x,y in meta_data['TYPE', 'VARIABLE'] if x == 'CATEGORIC'])
        ohe = OneHotEncoder(categorical_features=categorics)
        values = ohe.fit_transform(values)
        self.values = values
        return values
        
                
    
    def default_ratio(self, values=self.values, meta_data=self.meta_data, thresh=0.95, default=[0]):                         
        res = pd.DataFrame(columns=values.columns)
        a = default
        for i in values:
            if meta_data != None: 
                a = [x for x,y in meta_data['DEFAULT_VALUES', 'VARIABLE'] if y==i]
            res[i] = 1-(values[values[i].isin(a)].count()/len(values[i]))
            res_new = res[res[i]>thresh]
        return values[res_new], res.columns.difference(res_new.columns), res_new.columns

                         
    def normalize(self, values=self.values, thresh=0.1):
        values_norm = stats.zscore(values.select_dtypes(include=numerics().num))
        b = np.argwhere(np.abs(values_norm)>3)
        grouped_b = [list(j) for i, j in groupby(b[:,0])]
        values_norm = pd.DataFrame(values_norm)
        values_norm.columns = values.columns
        for i in range(len(grouped_b)):    
            if (len(grouped_b[i])/values_norm.shape[1]) >thresh:
                values_norm = values_norm.drop(grouped_b[i][0], axis=0)
        return values_norm

    def reduct_low_var(self, values=self.values, threshold):
        values = values.select_dtypes(include=numerics().num)
        var = values.var(skipna = True)
        res = []
        for v in range(len(var)):
            if var[v] >= threshold:
                res.append(var.index.values[v]) 
        features_eliminated = choose_eliminated(values, res)    
        return values[res], features_eliminated, res

    def reduct_dcor(self, values = self.values, n_features=100): '''formatı hakkında fikir geliştirilmeli kolaylaştırıcı (fazla radikal)'''
        values = values.select_dtypes(include=numerics().num)
        corr = np.zeros([len(values.columns),2])

        for i in range(len(values.columns)):
            corr[i,:] = np.array([i, dcor.distance_correlation(values.iloc[:,i], target)])
        
        if n_features > len(corr):
            features_chosen = values.columns[corr[:,0]]
        else:    
            corr = pd.DataFrame(corr).sort_values(by=corr.columns[1], ascending=False).iloc[:n_features,:] 
            corr = np.array(corr)
            kmeans = km(n_clusters = n_features).fit(corr[:,1].reshape(-1,1))
            labels = kmeans.labels_

            x=[]
            for l in range(n_features):
                x.append([i for i,d in enumerate(labels) if d==l])

            features_chosen = np.zeros(n_features)
            features_chosen = features_chosen-1
            maxx = np.zeros(n_features)
            for l in range(len(labels)):
                a = abs(dcor.distance_correlation(values.iloc[:,x[labels[l]]], values.iloc[:,int(corr[l,0])]))
                if a >= maxx[labels[l]]:
                    maxx[labels[l]] = a
                    features_chosen[labels[l]] = corr[l,0]

            features_chosen = np.array([int(i) for i in features_chosen if i>=0])
            features_chosen = values.columns[features_chosen]

        features_eliminated = choose_eliminated(values, features_chosen)
        return values[features_chosen], features_eliminated, features_chosen
     
    def reduct_dcor_sqr(self, values=self.values, n_features=100):
        values = values.select_dtypes(include=numerics().num)
        corr = np.zeros([len(values.columns),2])

        for i in range(len(values.columns)):
            corr[i,:] = np.array([i, dcor.u_distance_correlation_sqr(values.iloc[:,i], target)])
        
        if n_features > len(corr):
            features_chosen = values.columns[corr[:,0]]
        else:    
            corr = pd.DataFrame(corr).sort_values(by=corr.columns[1], ascending=False).iloc[:n_features,:] 
            corr = np.array(corr)
            
            kmeans = km(n_clusters = n_features).fit(corr[:,1].reshape(-1,1))
            labels = kmeans.labels_

            x=[]
            for l in range(n_features):
                x.append([i for i,d in enumerate(labels) if d==l])

            features_chosen = np.zeros(n_features)
            features_chosen = features_chosen-1
            maxx = np.zeros(n_features)
            for l in range(len(labels)):
                a = abs(dcor.u_distance_correlation_sqr(values.iloc[:,x[labels[l]]], values.iloc[:,int(corr[l,0])]))
                if a >= maxx[labels[l]]:
                    maxx[labels[l]] = a
                    features_chosen[labels[l]] = corr[l,0]

            features_chosen = np.array([int(i) for i in features_chosen if i>=0])
            features_chosen = values.columns[features_chosen]

        features_eliminated = choose_eliminated(values, features_chosen)
        return values[features_chosen], features_eliminated, features_chosen

    def choose_eliminated(values, features_chosen):
        values = values.drop(features_chosen, axis=1)
        features_eliminated = values.columns
        return features_eliminated

    def assign_zero(self, values=self.values):
        values = values.select_dtypes(include=numerics().num)
        values_new = values.dropna(thresh=int(values.shape[1]*0.95))
        values_new = values_new.fillna(0)
        values_new = values_new.drop([i for i in values_new.columns if values_new[i].sum()==0], axis = 1)
        return values_new, values.columns.difference(values_new.columns), values_new.columns

    def reduct_boostedtree(self, values=self.values, target=self.target, algtype, nb_features=100):
        model = algtype.value
        parameters = {'C': scipy.stats.expon(scale=100), 'gamma': scipy.stats.expon(scale=.1), 'kernel': ['rbf'], 'class_weight':['balanced', None]}
        clf = rscv(model, parameters)
        clf.fit(values, target)
        score = clf.best_score_
        model.fit(values, target, clf.best_params_)
        
        feature_weights = model.feature_importances_
        
        feature_weights_new = pd.DataFrame(feature_weights).sort_values(by=feature_weights.columns[1], ascending=False).iloc[:nb_features,:] 
        feature_weights_new = np.array(feature_weights)
        
        features_chosen = values.columns[np.transpose(np.argwhere(feature_weights_new))[0].tolist()]
        features_eliminated = choose_eliminated(values, features_chosen)
        return values[features_chosen], features_eliminated, features_chosen, score, feature_weights
      
    #def l2_regularization(values, target, degree = 10, alpha = 10):
    #    model = Pipeline([('poly', PolynomialFeatures(degree=degree)), ('l2', Ridge(alpha=alpha))])
    #    model = model.fit(values, target)
    #    model.named_steps['l2'].coef_
    #    output_features = model.get_feature_names(input_features = values.columns)
    #    return output_features  # i have no idea what is going to pop up because of this code


    def calculate_psi(expected, actual, buckettype='bins', buckets=10, axis=0):
        '''Calculate the PSI (population stability index) across all variables
        Args:
           expected: numpy matrix of original values
           actual: numpy matrix of new values, same size as expected
           buckettype: type of strategy for creating buckets, bins splits into even splits, quantiles splits into quantile buckets
           buckets: number of quantiles to use in bucketing variables
           axis: axis by which variables are defined, 0 for vertical, 1 for horizontal
        Returns:
           psi_values: ndarray of psi values for each variable
        Author:
           Matthew Burke
           github.com/mwburke
           worksofchart.com
        '''

        def psi(expected_array, actual_array, buckets):
            '''Calculate the PSI for a single variable
            Args:
               expected_array: numpy array of original values
               actual_array: numpy array of new values, same size as expected
               buckets: number of percentile ranges to bucket the values into
            Returns:
               psi_value: calculated PSI value
            '''

            def scale_range (input, min, max):
                input += -(np.min(input))
                input = input/(np.max(input) / (max - min))
                input += min
                return input


            breakpoints = np.arange(0, buckets + 1) / (buckets) * 100

            if buckettype == 'bins':
                breakpoints = scale_range(breakpoints, np.min(expected_array), np.max(expected_array))
            elif buckettype == 'quantiles':
                breakpoints = np.stack([np.percentile(expected_array, b) for b in breakpoints])



            expected_percents = np.histogram(expected_array, breakpoints)[0] / len(expected_array)
            actual_percents = np.histogram(actual_array, breakpoints)[0] / len(actual_array)

            def sub_psi(e_perc, a_perc):
                '''Calculate the actual PSI value from comparing the values.
                   Update the actual value to a very small number if equal to zero
                '''
                if a_perc == 0:
                    a_perc = 0.0001
                if e_perc == 0:
                    e_perc = 0.0001

                value = (e_perc - a_perc) * np.log(e_perc / a_perc)
                return(value)

            psi_value = np.sum(sub_psi(expected_percents[i], actual_percents[i]) for i in range(0, len(expected_percents)))

            return(psi_value)

        if len(expected.shape) == 1:
            psi_values = np.empty(len(expected.shape))
        else:
            psi_values = np.empty(expected.shape[axis])

        for i in range(0, len(psi_values)):
            if len(psi_values) == 1:
                psi_values = psi(expected, actual, buckets)
            elif axis == 0:
                psi_values[i] = psi(expected[:,i], actual[:,i], buckets)
            elif axis == 1:
                psi_values[i] = psi(expected[i,:], actual[i,:], buckets)

        return(psi_values)


    def calculate_all_psi(expected,acual,buckets=10, ax=0):
        inputs_expected = np.array(expected)
        inpts_actual =    np.array(acual)

        psi_all=pd.DataFrame(columns=['VARIABLE_INDEX','PSI_SCORE'])

        for i in range(inputs_expected.shape[1]):
            psi_i=calculate_psi(inputs_expected[:,i], inpts_actual[:,i], buckettype='bins', buckets=buckets, axis=ax)
            psi_all.loc[i]=[i,psi_i]       

        return psi_all

    def reduct_psi(self, values=self.values, test=self.test, thresh = 0.8):
        psi_all = calculate_all_psi(values, test)
        #thresh = max(np.percentile(psi_all['PSI_SCORE'], 10), ((max(psi_all['PSI_SCORE'])-min(psi_all['PSI_SCORE']))/thresh)+min(psi_all['PSI_SCORE']))
        features_chosen = [int(psi_all['VARIABLE_INDEX'][i]) for i in range(psi_all.shape[0]) if psi_all['PSI_SCORE'][i]<=thresh]
        features_eliminated = choose_eliminated(values, values.columns[features_chosen])
        return values[features_chosen], features_eliminated, features_chosen

 #   def rmsle(y, y_pred):    
 #       assert len(y) == len(y_pred)
 #       terms_to_sum = [(math.log(y_pred[i] + 1) - math.log(y[i] + 1)) ** 2.0 for i,pred in enumerate(y_pred)]
 #       return ((sum(terms_to_sum) * (1.0/len(y))) ** 0.5)

    def reduct_sparse_rp(self, values=self.values, test=self.test, eps): '''sütunlar kayboluyor doğal olarak'''
        values_new = 'test data is required!'
        if test != None:
            transformer = SparseRandomProjection(eps=eps)
            values_new = transformer.fit_transform(values)
        return values_new
      
    def solid(self, values=self.values, target=self.target, test=self.test, threshold=0, model_type):
        if model_type==1:
            a = self.reduct_boostedtree(values=self.values, target=self.target, algtype='xgboost_regressor', nb_features=100)
            b = self.reduct_boostedtree(values=self.values, target=self.target, algtype='lightgbm_regressor', nb_features=100)
            c = self.reduct_boostedtree(values=self.values, target=self.target, algtype='catboost_regressor', nb_features=100)
            res = np.array(a[-1])/a[-2] + np.array(b[-1])/b[-2] + np.array(c[-1])/c[-2]
            res = res[res>threshold]
        elif model_type==2:
            a = self.reduct_boostedtree(values=self.values, target=self.target, algtype='xgboost_classifier', nb_features=100)
            b = self.reduct_boostedtree(values=self.values, target=self.target, algtype='lightgbm_classifier', nb_features=100)
            c = self.reduct_boostedtree(values=self.values, target=self.target, algtype='catboost_classifier', nb_features=100)
            res = np.array(a[-1])/a[-2] + np.array(b[-1])/b[-2] + np.array(c[-1])/c[-2]
            res = res[res>threshold]
        return values[res], values.columns.difference(values[res].columns), res.index 
        
