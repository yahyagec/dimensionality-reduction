{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "union.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yahyagec/dimensionality-reduction/blob/master/union.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "5Frbm8ZkjwWa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install dcor\n",
        "!pip install lightgbm\n",
        "!pip install xgboost\n",
        "!pip install catboost\n",
        "#!pip install tsfresh\n",
        "!pip install keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U3R-QrdbhvPi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.cross_validation import train_test_split as tts\n",
        "import dcor\n",
        "from sklearn.cluster import KMeans as km\n",
        "from scipy import stats\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.preprocessing import PolynomialFea1tures\n",
        "from sklearn.pipeline import Pipeline\n",
        "from itertools import groupby\n",
        "import math\n",
        "import io\n",
        "from sklearn.random_projection import SparseRandomProjection\n",
        "from multimethod import multimethod\n",
        "from sklearn.model_selection import RandomizedSearchCV as rscv\n",
        "\n",
        "class numerics():\n",
        "    def __init__(num =  ['int16', 'int32', 'int64', 'float16', 'float32', 'float64'])\n",
        "        self.num = num\n",
        "    \n",
        "class AlgType(Enum):\n",
        "    xgboost_regressor = XGBRegressor()\n",
        "    xgboost_classifier = XGBClassifier()\n",
        "    lightgbm_regressor = LGBMRegressor()\n",
        "    lightgbm_classifier = LGBMClassifier()\n",
        "    catboost_regressor = CatBoostRegressor()\n",
        "    catboost_classifier = CatBoostClassifier()\n",
        "    \n",
        "class DimensionalityReduction():\n",
        "    \n",
        "    def __init__(self, raw_data, target_name, meta_data=None):\n",
        "        self.raw_data = raw_data\n",
        "        assert(isinstance(target_name, str), 'target_name must be string!') \n",
        "        self.values = raw_data.drop([target_name], axis=1) \n",
        "        self.target = raw_data[target_name]\n",
        "        self.meta_data = meta_data\n",
        "        \n",
        "        \n",
        "#    def get_data(self, target_name):\n",
        "#        assert(isinstance(target_name, str), 'target_name must be string!')\n",
        "#        values = self.raw_data.drop([target_name], axis=1) \n",
        "#        target = self.raw_data[target_name]\n",
        "#        return values, target\n",
        "\n",
        "  \n",
        "    def encoder(self, values=self.values, meta_data=self.meta_data):\n",
        "        categorics = np.array([y for x,y in meta_data['TYPE', 'VARIABLE'] if x == 'CATEGORIC'])\n",
        "        ohe = OneHotEncoder(categorical_features=categorics)\n",
        "        values = ohe.fit_transform(values)\n",
        "        self.values = values\n",
        "        return values\n",
        "        \n",
        "                \n",
        "    \n",
        "    def default_ratio(self, values=self.values, meta_data=self.meta_data, thresh=0.95, default=[0]):                         \n",
        "        res = pd.DataFrame(columns=values.columns)\n",
        "        a = default\n",
        "        for i in values:\n",
        "            if meta_data != None: \n",
        "                a = [x for x,y in meta_data['DEFAULT_VALUES', 'VARIABLE'] if y==i]\n",
        "            res[i] = 1-(values[values[i].isin(a)].count()/len(values[i]))\n",
        "            res_new = res[res[i]>thresh]\n",
        "        return values[res_new], res.columns.difference(res_new.columns), res_new.columns\n",
        "\n",
        "                         \n",
        "    def normalize(self, values=self.values, thresh=0.1):\n",
        "        values_norm = stats.zscore(values.select_dtypes(include=numerics().num))\n",
        "        b = np.argwhere(np.abs(values_norm)>3)\n",
        "        grouped_b = [list(j) for i, j in groupby(b[:,0])]\n",
        "        values_norm = pd.DataFrame(values_norm)\n",
        "        values_norm.columns = values.columns\n",
        "        for i in range(len(grouped_b)):    \n",
        "            if (len(grouped_b[i])/values_norm.shape[1]) >thresh:\n",
        "                values_norm = values_norm.drop(grouped_b[i][0], axis=0)\n",
        "        return values_norm\n",
        "\n",
        "    def reduct_low_var(self, values=self.values, threshold):\n",
        "        values = values.select_dtypes(include=numerics().num)\n",
        "        var = values.var(skipna = True)\n",
        "        res = []\n",
        "        for v in range(len(var)):\n",
        "            if var[v] >= threshold:\n",
        "                res.append(var.index.values[v]) \n",
        "        features_eliminated = choose_eliminated(values, res)    \n",
        "        return values[res], features_eliminated, res\n",
        "\n",
        "    def reduct_dcor(self, values = self.values, n_features=100): '''formatı hakkında fikir geliştirilmeli kolaylaştırıcı (fazla radikal)'''\n",
        "        values = values.select_dtypes(include=numerics().num)\n",
        "        corr = np.zeros([len(values.columns),2])\n",
        "\n",
        "        for i in range(len(values.columns)):\n",
        "            corr[i,:] = np.array([i, dcor.distance_correlation(values.iloc[:,i], target)])\n",
        "        \n",
        "        if n_features > len(corr):\n",
        "            features_chosen = values.columns[corr[:,0]]\n",
        "        else:    \n",
        "            corr = pd.DataFrame(corr).sort_values(by=corr.columns[1], ascending=False).iloc[:n_features,:] \n",
        "            corr = np.array(corr)\n",
        "            kmeans = km(n_clusters = n_features).fit(corr[:,1].reshape(-1,1))\n",
        "            labels = kmeans.labels_\n",
        "\n",
        "            x=[]\n",
        "            for l in range(n_features):\n",
        "                x.append([i for i,d in enumerate(labels) if d==l])\n",
        "\n",
        "            features_chosen = np.zeros(n_features)\n",
        "            features_chosen = features_chosen-1\n",
        "            maxx = np.zeros(n_features)\n",
        "            for l in range(len(labels)):\n",
        "                a = abs(dcor.distance_correlation(values.iloc[:,x[labels[l]]], values.iloc[:,int(corr[l,0])]))\n",
        "                if a >= maxx[labels[l]]:\n",
        "                    maxx[labels[l]] = a\n",
        "                    features_chosen[labels[l]] = corr[l,0]\n",
        "\n",
        "            features_chosen = np.array([int(i) for i in features_chosen if i>=0])\n",
        "            features_chosen = values.columns[features_chosen]\n",
        "\n",
        "        features_eliminated = choose_eliminated(values, features_chosen)\n",
        "        return values[features_chosen], features_eliminated, features_chosen\n",
        "     \n",
        "    def reduct_dcor_sqr(self, values=self.values, n_features=100):\n",
        "        values = values.select_dtypes(include=numerics().num)\n",
        "        corr = np.zeros([len(values.columns),2])\n",
        "\n",
        "        for i in range(len(values.columns)):\n",
        "            corr[i,:] = np.array([i, dcor.u_distance_correlation_sqr(values.iloc[:,i], target)])\n",
        "        \n",
        "        if n_features > len(corr):\n",
        "            features_chosen = values.columns[corr[:,0]]\n",
        "        else:    \n",
        "            corr = pd.DataFrame(corr).sort_values(by=corr.columns[1], ascending=False).iloc[:n_features,:] \n",
        "            corr = np.array(corr)\n",
        "            \n",
        "            kmeans = km(n_clusters = n_features).fit(corr[:,1].reshape(-1,1))\n",
        "            labels = kmeans.labels_\n",
        "\n",
        "            x=[]\n",
        "            for l in range(n_features):\n",
        "                x.append([i for i,d in enumerate(labels) if d==l])\n",
        "\n",
        "            features_chosen = np.zeros(n_features)\n",
        "            features_chosen = features_chosen-1\n",
        "            maxx = np.zeros(n_features)\n",
        "            for l in range(len(labels)):\n",
        "                a = abs(dcor.u_distance_correlation_sqr(values.iloc[:,x[labels[l]]], values.iloc[:,int(corr[l,0])]))\n",
        "                if a >= maxx[labels[l]]:\n",
        "                    maxx[labels[l]] = a\n",
        "                    features_chosen[labels[l]] = corr[l,0]\n",
        "\n",
        "            features_chosen = np.array([int(i) for i in features_chosen if i>=0])\n",
        "            features_chosen = values.columns[features_chosen]\n",
        "\n",
        "        features_eliminated = choose_eliminated(values, features_chosen)\n",
        "        return values[features_chosen], features_eliminated, features_chosen\n",
        "\n",
        "    def choose_eliminated(values, features_chosen):\n",
        "        values = values.drop(features_chosen, axis=1)\n",
        "        features_eliminated = values.columns\n",
        "        return features_eliminated\n",
        "\n",
        "    def assign_zero(self, values=self.values):\n",
        "        values = values.select_dtypes(include=numerics().num)\n",
        "        values_new = values.dropna(thresh=int(values.shape[1]*0.95))\n",
        "        values_new = values_new.fillna(0)\n",
        "        values_new = values_new.drop([i for i in values_new.columns if values_new[i].sum()==0], axis = 1)\n",
        "        return values_new, values.columns.difference(values_new.columns), values_new.columns\n",
        "\n",
        "    def reduct_boostedtree(self, values=self.values, target=self.target, algtype, nb_features=100):\n",
        "        model = algtype.value\n",
        "        parameters = {'C': scipy.stats.expon(scale=100), 'gamma': scipy.stats.expon(scale=.1), 'kernel': ['rbf'], 'class_weight':['balanced', None]}\n",
        "        clf = rscv(model, parameters)\n",
        "        clf.fit(values, target)\n",
        "        score = clf.best_score_\n",
        "        model.fit(values, target, clf.best_params_)\n",
        "        \n",
        "        feature_weights = model.feature_importances_\n",
        "        \n",
        "        feature_weights_new = pd.DataFrame(feature_weights).sort_values(by=feature_weights.columns[1], ascending=False).iloc[:nb_features,:] \n",
        "        feature_weights_new = np.array(feature_weights)\n",
        "        \n",
        "        features_chosen = values.columns[np.transpose(np.argwhere(feature_weights_new))[0].tolist()]\n",
        "        features_eliminated = choose_eliminated(values, features_chosen)\n",
        "        return values[features_chosen], features_eliminated, features_chosen, score, feature_weights\n",
        "      \n",
        "    #def l2_regularization(values, target, degree = 10, alpha = 10):\n",
        "    #    model = Pipeline([('poly', PolynomialFeatures(degree=degree)), ('l2', Ridge(alpha=alpha))])\n",
        "    #    model = model.fit(values, target)\n",
        "    #    model.named_steps['l2'].coef_\n",
        "    #    output_features = model.get_feature_names(input_features = values.columns)\n",
        "    #    return output_features  # i have no idea what is going to pop up because of this code\n",
        "\n",
        "\n",
        "    def calculate_psi(expected, actual, buckettype='bins', buckets=10, axis=0):\n",
        "        '''Calculate the PSI (population stability index) across all variables\n",
        "        Args:\n",
        "           expected: numpy matrix of original values\n",
        "           actual: numpy matrix of new values, same size as expected\n",
        "           buckettype: type of strategy for creating buckets, bins splits into even splits, quantiles splits into quantile buckets\n",
        "           buckets: number of quantiles to use in bucketing variables\n",
        "           axis: axis by which variables are defined, 0 for vertical, 1 for horizontal\n",
        "        Returns:\n",
        "           psi_values: ndarray of psi values for each variable\n",
        "        Author:\n",
        "           Matthew Burke\n",
        "           github.com/mwburke\n",
        "           worksofchart.com\n",
        "        '''\n",
        "\n",
        "        def psi(expected_array, actual_array, buckets):\n",
        "            '''Calculate the PSI for a single variable\n",
        "            Args:\n",
        "               expected_array: numpy array of original values\n",
        "               actual_array: numpy array of new values, same size as expected\n",
        "               buckets: number of percentile ranges to bucket the values into\n",
        "            Returns:\n",
        "               psi_value: calculated PSI value\n",
        "            '''\n",
        "\n",
        "            def scale_range (input, min, max):\n",
        "                input += -(np.min(input))\n",
        "                input = input/(np.max(input) / (max - min))\n",
        "                input += min\n",
        "                return input\n",
        "\n",
        "\n",
        "            breakpoints = np.arange(0, buckets + 1) / (buckets) * 100\n",
        "\n",
        "            if buckettype == 'bins':\n",
        "                breakpoints = scale_range(breakpoints, np.min(expected_array), np.max(expected_array))\n",
        "            elif buckettype == 'quantiles':\n",
        "                breakpoints = np.stack([np.percentile(expected_array, b) for b in breakpoints])\n",
        "\n",
        "\n",
        "\n",
        "            expected_percents = np.histogram(expected_array, breakpoints)[0] / len(expected_array)\n",
        "            actual_percents = np.histogram(actual_array, breakpoints)[0] / len(actual_array)\n",
        "\n",
        "            def sub_psi(e_perc, a_perc):\n",
        "                '''Calculate the actual PSI value from comparing the values.\n",
        "                   Update the actual value to a very small number if equal to zero\n",
        "                '''\n",
        "                if a_perc == 0:\n",
        "                    a_perc = 0.0001\n",
        "                if e_perc == 0:\n",
        "                    e_perc = 0.0001\n",
        "\n",
        "                value = (e_perc - a_perc) * np.log(e_perc / a_perc)\n",
        "                return(value)\n",
        "\n",
        "            psi_value = np.sum(sub_psi(expected_percents[i], actual_percents[i]) for i in range(0, len(expected_percents)))\n",
        "\n",
        "            return(psi_value)\n",
        "\n",
        "        if len(expected.shape) == 1:\n",
        "            psi_values = np.empty(len(expected.shape))\n",
        "        else:\n",
        "            psi_values = np.empty(expected.shape[axis])\n",
        "\n",
        "        for i in range(0, len(psi_values)):\n",
        "            if len(psi_values) == 1:\n",
        "                psi_values = psi(expected, actual, buckets)\n",
        "            elif axis == 0:\n",
        "                psi_values[i] = psi(expected[:,i], actual[:,i], buckets)\n",
        "            elif axis == 1:\n",
        "                psi_values[i] = psi(expected[i,:], actual[i,:], buckets)\n",
        "\n",
        "        return(psi_values)\n",
        "\n",
        "\n",
        "    def calculate_all_psi(expected,acual,buckets=10, ax=0):\n",
        "        inputs_expected = np.array(expected)\n",
        "        inpts_actual =    np.array(acual)\n",
        "\n",
        "        psi_all=pd.DataFrame(columns=['VARIABLE_INDEX','PSI_SCORE'])\n",
        "\n",
        "        for i in range(inputs_expected.shape[1]):\n",
        "            psi_i=calculate_psi(inputs_expected[:,i], inpts_actual[:,i], buckettype='bins', buckets=buckets, axis=ax)\n",
        "            psi_all.loc[i]=[i,psi_i]       \n",
        "\n",
        "        return psi_all\n",
        "\n",
        "    def reduct_psi(self, values=self.values, test=self.test, thresh = 0.8):\n",
        "        psi_all = calculate_all_psi(values, test)\n",
        "        #thresh = max(np.percentile(psi_all['PSI_SCORE'], 10), ((max(psi_all['PSI_SCORE'])-min(psi_all['PSI_SCORE']))/thresh)+min(psi_all['PSI_SCORE']))\n",
        "        features_chosen = [int(psi_all['VARIABLE_INDEX'][i]) for i in range(psi_all.shape[0]) if psi_all['PSI_SCORE'][i]<=thresh]\n",
        "        features_eliminated = choose_eliminated(values, values.columns[features_chosen])\n",
        "        return values[features_chosen], features_eliminated, features_chosen\n",
        "\n",
        " #   def rmsle(y, y_pred):    \n",
        " #       assert len(y) == len(y_pred)\n",
        " #       terms_to_sum = [(math.log(y_pred[i] + 1) - math.log(y[i] + 1)) ** 2.0 for i,pred in enumerate(y_pred)]\n",
        " #       return ((sum(terms_to_sum) * (1.0/len(y))) ** 0.5)\n",
        "\n",
        "    def reduct_sparse_rp(self, values=self.values, test=self.test, eps): '''sütunlar kayboluyor doğal olarak'''\n",
        "        values_new = 'test data is required!'\n",
        "        if test != None:\n",
        "            transformer = SparseRandomProjection(eps=eps)\n",
        "            values_new = transformer.fit_transform(values)\n",
        "        return values_new\n",
        "      \n",
        "    def solid(self, values=self.values, target=self.target, test=self.test, threshold=0, model_type):\n",
        "        if model_type==1:\n",
        "            a = self.reduct_boostedtree(values=self.values, target=self.target, algtype='xgboost_regressor', nb_features=100)\n",
        "            b = self.reduct_boostedtree(values=self.values, target=self.target, algtype='lightgbm_regressor', nb_features=100)\n",
        "            c = self.reduct_boostedtree(values=self.values, target=self.target, algtype='catboost_regressor', nb_features=100)\n",
        "            res = np.array(a[-1])/a[-2] + np.array(b[-1])/b[-2] + np.array(c[-1])/c[-2]\n",
        "            res = res[res>threshold]\n",
        "        elif model_type==2:\n",
        "            a = self.reduct_boostedtree(values=self.values, target=self.target, algtype='xgboost_classifier', nb_features=100)\n",
        "            b = self.reduct_boostedtree(values=self.values, target=self.target, algtype='lightgbm_classifier', nb_features=100)\n",
        "            c = self.reduct_boostedtree(values=self.values, target=self.target, algtype='catboost_classifier', nb_features=100)\n",
        "            res = np.array(a[-1])/a[-2] + np.array(b[-1])/b[-2] + np.array(c[-1])/c[-2]\n",
        "            res = res[res>threshold]\n",
        "        return values[res], values.columns.difference(values[res].columns), res.index \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zgi-RS2mCXUi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Install a Drive FUSE wrapper.\n",
        "# https://github.com/astrada/google-drive-ocamlfuse\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "!pip install --upgrade -q gspread\n",
        "import gspread\n",
        "# Generate auth tokens for Colab\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Generate creds for the Drive FUSE library.\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "gc = gspread.authorize(creds)\n",
        "import getpass\n",
        "# Work around misordering of STREAM and STDIN in Jupyter.\n",
        "# https://github.com/jupyter/notebook/issues/3159\n",
        "prompt = !google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass(prompt[0] + '\\n\\nEnter verification code: ')\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5J2Uip8vFaM2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create a directory and mount Google Drive using that directory.\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive\n",
        "\n",
        "test = pd.read_csv('drive/ColabNotebooks/test.csv')\n",
        "train = pd.read_csv('drive/ColabNotebooks/train.csv')\n",
        "\n",
        "# Create a file in Drive.\n",
        "!echo \"This newly created file will appear in your Drive file list.\" > drive/created.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j0d_H7G3V07N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print((res.iloc[:,0]<0).count())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wcIj5vvlv9ms",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "values, target = get_data(train)\n",
        "values = assign_zero(values)\n",
        "#col = values.columns\n",
        "#values = reduct_sparse_rp(values, 0.3)\n",
        "#values = pd.DataFrame(values)\n",
        "#values.columns = col[:values.shape[1]]\n",
        "values_train, values_test, target_train, target_test = tts(values, target, test_size=0.2)\n",
        "\n",
        "a = reduct_low_var(values_train, 0.001)\n",
        "values_train = values_train.drop(a, axis=1)\n",
        "print(\"reductedby_low_var completed: \", a)\n",
        "#b = reduct_dcor(values_train, target_train)\n",
        "#reductedby_dcor = values_train.drop(b ,axis=1)\n",
        "#print(\"reductedby_dcor completed: \", b)\n",
        "#c = reduct_dcor_sqr(values_train, target_train)\n",
        "#reductedby_dcor_sqr = values_train.drop(c, axis=1)\n",
        "#print(\"reductedby_dcor_sqr completed: \", c)\n",
        "#d = reduct_psi(values_train, test[values_train.columns])\n",
        "#reductedby_psi = values_train.drop(d, axis=1)\n",
        "#print(\"reductedby_psi completed: \", d)\n",
        "e = reduct_xgbregressor(values_train, target_train)\n",
        "values_train = values_train.drop(e, axis=1)\n",
        "print(\"reductedby_xgbregressor completed: \", e)\n",
        "#b = reduct_dcor(values_train, target_train)\n",
        "#reductedby_dcor = values_train.drop(b ,axis=1)\n",
        "#print(\"reductedby_dcor completed: \", b)\n",
        "c = reduct_dcor_sqr(values_train, target_train)\n",
        "values_train = values_train.drop(c, axis=1)\n",
        "print(\"reductedby_dcor_sqr completed: \", c)\n",
        "#f = choose_eliminated(values_train, l2_regularization(values_train, target_train))\n",
        "#reductedby_l2 = values_train.drop(f, axis=1)\n",
        "#print(\"reductedby_l2 completed: \", f)\n",
        "\n",
        "#a = 0\n",
        "res = pd.DataFrame(columns = ['values_train'])\n",
        "\n",
        "#for i in [reductedby_low_var, reductedby_dcor, reductedby_dcor_sqr, reductedby_psi, reductedby_xgbregressor]:\n",
        "#    print(\"exporting: \", res.columns[i])\n",
        "#    file_name = str(res.columns[a] + 'sparse')\n",
        "#    sh = gc.create(file_name)\n",
        "#    worksheet = gc.open(file_name).sheet1\n",
        "#    cell_list = worksheet.range(1, 1, reducedby_low_var.shape[1], 1)\n",
        "#    a = 0\n",
        "#    for cell in cell_list:\n",
        "#        cell.value = i[a]\n",
        "#        a+=1\n",
        "#    worksheet.update_cells(cell_list)\n",
        "\n",
        "rmsl = []    \n",
        "a=0    \n",
        "for i in [values_train]:\n",
        "    model = XGBRegressor()\n",
        "    model.fit(i, target_train)\n",
        "    res.iloc[:,a] = np.array(model.predict(values_test[i.columns]))\n",
        "    rmsl.append(rmsle(target_test.tolist(), res.iloc[:,a].tolist()))\n",
        "    a += 1\n",
        "    \n",
        "print(rmsl)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JUYqEqFlsmvh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "values, target = get_data(train)\n",
        "values_train, values_test, target_train, target_test = tts(values, target, test_size=0.2)\n",
        "values_train = assign_zero(values_train)\n",
        "values_train = normalize(values_train, 0.2)\n",
        "values_test = values_test[values_train.columns]\n",
        "\n",
        "a = reduct_low_var(values_train, 0.001)\n",
        "reductedby_low_var = values_train.drop(a, axis=1)\n",
        "print(\"reductedby_low_var completed: \", a)\n",
        "#b = reduct_dcor(values_train, target_train)\n",
        "#reductedby_dcor = values_train.drop(b ,axis=1)\n",
        "#print(\"reductedby_dcor completed: \", b)\n",
        "#c = reduct_dcor_sqr(values_train, target_train)\n",
        "#reductedby_dcor_sqr = values_train.drop(c, axis=1)\n",
        "#print(\"reductedby_dcor_sqr completed: \", c)\n",
        "d = reduct_psi(values_train, test[values_train.columns])\n",
        "reductedby_psi = values_train.drop(d, axis=1)\n",
        "print(\"reductedby_psi completed: \", d)\n",
        "e = reduct_xgbregressor(values_train, target_train)\n",
        "reductedby_xgbregressor = values_train.drop(e, axis=1)\n",
        "print(\"reductedby_xgbregressor completed: \", e)\n",
        "#f = choose_eliminated(values_train, l2_regularization(values_train, target_train))\n",
        "#reductedby_l2 = values_train.drop(f, axis=1)\n",
        "#print(\"reductedby_l2 completed: \", f)\n",
        "\n",
        "a = 0\n",
        "res = pd.DataFrame(columns = ['reductedby_low_var',''' 'reductedby_dcor', 'reductedby_dcor_sqr',''' 'reductedby_psi', 'reductedby_xgbregressor'])\n",
        "\n",
        "for i in [reductedby_low_var, '''reductedby_dcor, reductedby_dcor_sqr,''' reductedby_psi, reductedby_xgbregressor]:\n",
        "    print(\"exporting: \", res.columns[i])\n",
        "    sh = gc.create(res.columns[a])\n",
        "    worksheet = gc.open(res.columns[a]).sheet1\n",
        "    cell_list = worksheet.range(1, 1, reducedby_low_var.shape[1], 1)\n",
        "    a = 0\n",
        "    for cell in cell_list:\n",
        "        cell.value = i[a]\n",
        "        a+=1\n",
        "    worksheet.update_cells(cell_list)\n",
        "\n",
        "    \n",
        "rmsle = []    \n",
        "a=0    \n",
        "for i in [reductedby_low_var, reductedby_dcor, reductedby_dcor_sqr, reductedby_psi, reductedby_xgbregressor]:\n",
        "    print(\"modeling: \", res.columns[i])    \n",
        "    model = XGBRegressor()\n",
        "    model.fit(i, target_train)\n",
        "    res.iloc[:,a] = model.predict(values_test[i.columns])\n",
        "    rmsle.append(rmsle(target_test, res.iloc[:,a]))\n",
        "    a += 1\n",
        "    \n",
        "print(rmsle)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5KA-U5oiSime",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "values, target = get_data(train)\n",
        "values_train, values_test, target_train, target_test = tts(values, target, test_size=0.2)\n",
        "values_train = assign_zero(values_train)\n",
        "values_train = normalize(values_train, 0.2)\n",
        "values_test = values_test[values_train.columns]\n",
        "\n",
        "a = reduct_low_var(values_train, 0.001)\n",
        "reductedby_low_var = values_train.drop(a, axis=1)\n",
        "print(\"reductedby_low_var completed: \", a)\n",
        "b = reduct_dcor(values_train, target_train)\n",
        "reductedby_dcor = values_train.drop(b ,axis=1)\n",
        "print(\"reductedby_dcor completed: \", b)\n",
        "c = reduct_dcor_sqr(values_train, target_train)\n",
        "reductedby_dcor_sqr = values_train.drop(c, axis=1)\n",
        "print(\"reductedby_dcor_sqr completed: \", c)\n",
        "d = reduct_psi(values_train, test[values_train.columns])\n",
        "reductedby_psi = values_train.drop(d, axis=1)\n",
        "print(\"reductedby_psi completed: \", d)\n",
        "e = reduct_xgbregressor(values_train, target_train)\n",
        "reductedby_xgbregressor = values_train.drop(e, axis=1)\n",
        "print(\"reductedby_xgbregressor completed: \", e)\n",
        "#f = choose_eliminated(values_train, l2_regularization(values_train, target_train))\n",
        "#reductedby_l2 = values_train.drop(f, axis=1)\n",
        "#print(\"reductedby_l2 completed: \", f)\n",
        "\n",
        "a = 0\n",
        "\n",
        "for i in [reductedby_low_var, reductedby_dcor, reductedby_dcor_sqr, reductedby_psi, reductedby_xgbregressor]:\n",
        "    print(\"exporting: \", res.columns[i])\n",
        "    sh = gc.create(res.columns[a])\n",
        "    worksheet = gc.open(res.columns[a]).sheet1\n",
        "    cell_list = worksheet.range(1, 1, i.shape[1], 1)\n",
        "    a = 0\n",
        "    for cell in cell_list:\n",
        "        cell.value = i[a]\n",
        "        a+=1\n",
        "    worksheet.update_cells(cell_list)\n",
        "\n",
        "lll = [reductedby_low_var, reductedby_dcor, reductedby_dcor_sqr, reductedby_psi, reductedby_xgbregressor]\n",
        "\n",
        "fin = pd.DataFrame(np.ones([len(values.columns),len(lll)]))\n",
        "fin.index = values.columns\n",
        "fin.columns = ['reductedby_low_var', 'reductedby_dcor', 'reductedby_dcor_sqr', 'reductedby_psi', 'reductedby_xgbregressor']\n",
        "\n",
        "a = 0\n",
        "for l in lll:\n",
        "    fin.iloc[l.columns,a] = np.zeros(len(l.columns))\n",
        "    a+=1\n",
        "    \n",
        "sh = gc.create('fin')\n",
        "worksheet = gc.open('fin').sheet1\n",
        "for z in range(len(lll)):\n",
        "    cell_list = worksheet.range(1, z+1, fin.shape[1], z+1)\n",
        "    a = 0\n",
        "    for cell in cell_list:\n",
        "        cell.value = fin.iloc[a,z]\n",
        "        a+=1\n",
        "\n",
        "        worksheet.update_cells(cell_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_yUBnD_HYWfy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from xgboost import XGB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "\n",
        "\n",
        "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
        "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
        "    \"\"\"\n",
        "    Generate a simple plot of the test and training learning curve.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
        "        An object of that type which is cloned for each validation.\n",
        "\n",
        "    title : string\n",
        "        Title for the chart.\n",
        "\n",
        "    X : array-like, shape (n_samples, n_features)\n",
        "        Training vector, where n_samples is the number of samples and\n",
        "        n_features is the number of features.\n",
        "\n",
        "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
        "        Target relative to X for classification or regression;\n",
        "        None for unsupervised learning.\n",
        "\n",
        "    ylim : tuple, shape (ymin, ymax), optional\n",
        "        Defines minimum and maximum yvalues plotted.\n",
        "\n",
        "    cv : int, cross-validation generator or an iterable, optional\n",
        "        Determines the cross-validation splitting strategy.\n",
        "        Possible inputs for cv are:\n",
        "          - None, to use the default 3-fold cross-validation,\n",
        "          - integer, to specify the number of folds.\n",
        "          - An object to be used as a cross-validation generator.\n",
        "          - An iterable yielding train/test splits.\n",
        "\n",
        "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
        "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
        "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
        "\n",
        "        Refer :ref:`User Guide <cross_validation>` for the various\n",
        "        cross-validators that can be used here.\n",
        "\n",
        "    n_jobs : integer, optional\n",
        "        Number of jobs to run in parallel (default 1).\n",
        "    \"\"\"\n",
        "    plt.figure()\n",
        "    plt.title(title)\n",
        "    if ylim is not None:\n",
        "        plt.ylim(*ylim)\n",
        "    plt.xlabel(\"Training examples\")\n",
        "    plt.ylabel(\"Score\")\n",
        "    train_sizes, train_scores, test_scores = learning_curve(\n",
        "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
        "    train_scores_mean = np.mean(train_scores, axis=1)\n",
        "    train_scores_std = np.std(train_scores, axis=1)\n",
        "    test_scores_mean = np.mean(test_scores, axis=1)\n",
        "    test_scores_std = np.std(test_scores, axis=1)\n",
        "    plt.grid()\n",
        "\n",
        "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
        "                     color=\"r\")\n",
        "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
        "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
        "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
        "             label=\"Training score\")\n",
        "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
        "             label=\"Cross-validation score\")\n",
        "\n",
        "    plt.legend(loc=\"best\")\n",
        "    return plt\n",
        "\n",
        "\n",
        "title = \"Learning Curves\"\n",
        "cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)\n",
        "\n",
        "estimator = XGBRegressor()\n",
        "plot_learning_curve(estimator, title, values_train, target_train, ylim=(0.7, 1.01), cv=cv, n_jobs=4)\n",
        "\n",
        "title = \"Learning Curves (SVM, RBF kernel, $\\gamma=0.001$)\"\n",
        "# SVC is more expensive so we do a lower number of CV iterations:\n",
        "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
        "estimator = SVC(gamma=0.001)\n",
        "plot_learning_curve(estimator, title, values_train, target_train, (0.7, 1.01), cv=cv, n_jobs=4)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k5qM8HTzMfkR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "t = 0\n",
        "lis =  ['reductedby_low_var', 'reductedby_dcor', 'reductedby_dcor_sqr']\n",
        "for i in [reductedby_low_var, reductedby_dcor, reductedby_dcor_sqr]:\n",
        "    sh = gc.create(lis[t])\n",
        "    worksheet = gc.open(lis[t]).sheet1\n",
        "    cell_list = worksheet.range(1, 1, i.shape[1], 1)\n",
        "    a = 0\n",
        "    for cell in cell_list:\n",
        "        cell.value = i.columns[a]\n",
        "        a+=1\n",
        "    worksheet.update_cells(cell_list)\n",
        "    t+=1\n",
        "\n",
        "lll = [reductedby_low_var, reductedby_dcor, reductedby_dcor_sqr]\n",
        "\n",
        "fin = pd.DataFrame(np.ones([len(values.columns),len(lll)]))\n",
        "fin.index = values.columns\n",
        "fin.columns = lis\n",
        "\n",
        "a = 0\n",
        "for l in lll:\n",
        "    fin.loc[l.columns, [lis[a]]] = 0\n",
        "    a+=1\n",
        "    \n",
        "sh = gc.create('fin')\n",
        "worksheet = gc.open('fin').sheet1\n",
        "for z in range(len(lll)):\n",
        "    cell_list = worksheet.range(1, z+1, fin.shape[0], z+1)\n",
        "    a = 0\n",
        "    for cell in cell_list:\n",
        "        cell.value = fin.iloc[a,z]\n",
        "        a+=1\n",
        "\n",
        "        worksheet.update_cells(cell_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h5DyfPK2dSpq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "fin.to_csv('fin.csv')\n",
        "files.download('fin.csv')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D6E4NuY6f7zS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "values, target = get_data(train)\n",
        "values_train, values_test, target_train, target_test = tts(values, target, test_size=0.2)\n",
        "values_train = assign_zero(values_train)\n",
        "values_train = normalize(values_train, 0.2)\n",
        "values_test = values_test[values_train.columns]\n",
        "\n",
        "a = reduct_low_var(values_train, 0.001)\n",
        "reductedby_low_var = values_train.drop(a, axis=1)\n",
        "'''b = reduct_dcor(values_train, target_train)\n",
        "reductedby_dcor = values_train.drop(b ,axis=1)\n",
        "c = reduct_dcor_sqr(values_train, target_train)\n",
        "reductedby_dcor_sqr = values_train.drop(c, axis=1)'''\n",
        "d = reduct_psi(values_train, test[values_train.columns], thresh=0.25)\n",
        "reductedby_psi = values_train.drop(d, axis=1)\n",
        "e = reduct_xgbregressor(values_train, target_train)\n",
        "reductedby_xgbregressor = values_train.drop(e, axis=1)\n",
        "\n",
        "#f = choose_eliminated(values_train, l2_regularization(values_train, target_train))\n",
        "#reductedby_l2 = values_train.drop(f, axis=1)\n",
        "#print(\"reductedby_l2 completed: \", f)\n",
        "\n",
        "lis = ['reductedby_low_var', 'reductedby_psi', 'reductedby_xgbregressor', 'reductedby_dcor', 'reductedby_dcor_sqr']\n",
        "lll = [reductedby_low_var, reductedby_psi, reductedby_xgbregressor]\n",
        "\n",
        "fin = pd.DataFrame(np.ones([len(values.columns),5]))\n",
        "fin.index = values.columns\n",
        "fin.columns = lis\n",
        "fin['reductedby_dcor'] = np.array(reds['reductedby_dcor'])\n",
        "fin['reductedby_dcor_sqr'] = np.array(reds['reductedby_dcor_sqr'])\n",
        "\n",
        "a = 0\n",
        "for l in lll:\n",
        "    fin.loc[l.columns, [lis[a]]] = 0\n",
        "    a+=1\n",
        "    \n",
        "fin.to_csv('fin.csv')\n",
        "files.download('fin.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ypaQBY3hl5Hz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X10A29AevfX3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "\n",
        "reds = pd.read_csv(io.StringIO(uploaded['dimensionality reduction.csv'].decode('utf-8')))\n",
        "#reds.index = reds.iloc[:,0]\n",
        "\n",
        "#a = [i for i in reds.index if reds.loc[i,'reductedby_low_var']==0]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dzN-adgE-emZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(values_train['cde9c35e8'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sLQSyvYIw-eL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#from xgboost import XGBRegressor\n",
        "\n",
        "#values, target = get_data(train)\n",
        "#values_train, values_test, target_train, target_test = tts(values, target, test_size=0.2)\n",
        "#values_train = assign_zero(values_train)\n",
        "#values_train = normalize(values_train, 0.2)\n",
        "#values_test = values_test[values_train.columns]\n",
        "\n",
        "model = XGBRegressor()\n",
        "\n",
        "model.fit(values_train[a], target_train)\n",
        "res = model.predict(values_test[a])\n",
        "rmsle_ = rmsle(target_test.tolist(), res)\n",
        "\n",
        "print(rmsle_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "raJreATZCmL8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "a = [i for i in fin.index if fin.loc[i,'reductedby_dcor']==0]\n",
        "print(a)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8BF31Ul_qqc1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rmsl = []    \n",
        "res = pd.DataFrame(columns = ['reductedby_low_var',  'reductedby_psi', 'reductedby_xgbregressor', 'reductedby_dcor', 'reductedby_dcor_sqr', 'values_train', 'aaa'])\n",
        "a=0\n",
        "reductedby_dcor = values_train[[i for i in fin.index if fin.loc[i,'reductedby_dcor']==0]]\n",
        "reductedby_dcor_sqr = values_train[[i for i in fin.index if fin.loc[i,'reductedby_dcor_sqr']==0]]\n",
        "aaa, values_test, target_train, target_test = tts(values, target, test_size=0.2)\n",
        "for i in [reductedby_low_var, reductedby_dcor, reductedby_dcor_sqr, reductedby_psi, reductedby_xgbregressor, values_train, aaa]:   \n",
        "    model = XGBRegressor()\n",
        "    model.fit(i, target_train)\n",
        "    res.iloc[:,a] = model.predict(values_test[i.columns])\n",
        "    rmsl.append(rmsle(target_test.tolist(), res.iloc[:,a].tolist()))\n",
        "    a += 1\n",
        "    \n",
        "print(rmsl)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lz1-Lr6Z7-bY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rmsl = 5\n",
        "n_opt_j = -1\n",
        "n_opt_i = -1\n",
        "\n",
        "values, target = get_data(train)\n",
        "values_train, values_test, target_train, target_test = tts(values, target, test_size=0.2)\n",
        "values_train = values_train.fillna(0)\n",
        "\n",
        "corr = np.zeros([len(values_train.columns),2])\n",
        "    \n",
        "for i in range(len(values_train.columns)):\n",
        "    corr[i,:] = np.array([i, dcor.u_distance_correlation_sqr(values_train.iloc[:,i], target_train)])\n",
        "    \n",
        "for j in range(5):\n",
        "    thresh = np.nanpercentile(corr[:,1], 10+(4*j))\n",
        "    corr_new = np.array([i for i in corr if i[1] >= thresh])   \n",
        "    for i in range(7):\n",
        "        n_features = 4*(2**i)\n",
        "        if n_features > len(corr_new):\n",
        "            features_chosen = values_train.columns[corr_new[:,0]]\n",
        "            features_eliminated = choose_eliminated(values_train, features_chosen)\n",
        "            re = values_train.drop(features_eliminated, axis = 1)\n",
        "           \n",
        "        else:     \n",
        "            kmeans = km(n_clusters = n_features).fit(corr_new[:,1].reshape(-1,1))\n",
        "            labels = kmeans.labels_\n",
        "\n",
        "            x=[]\n",
        "            for l in range(n_features):\n",
        "                x.append([m for m,d in enumerate(labels) if d==l])\n",
        "            features_chosen = np.zeros(n_features)\n",
        "            features_chosen = features_chosen-1\n",
        "            maxx = np.zeros(n_features)\n",
        "            for l in range(len(labels)):\n",
        "                a = abs(dcor.u_distance_correlation_sqr(values_train.iloc[:,x[labels[l]]], values_train.iloc[:,int(corr_new[l,0])]))\n",
        "                if a >= maxx[labels[l]]:\n",
        "                    maxx[labels[l]] = a\n",
        "                    features_chosen[labels[l]] = corr_new[l,0]\n",
        "            features_chosen = np.array([int(z) for z in features_chosen if z>=0])\n",
        "            features_chosen = values.columns[features_chosen]\n",
        "            features_eliminated = choose_eliminated(values_train, features_chosen)   \n",
        "            re = values_train.drop(features_eliminated, axis = 1)\n",
        "            \n",
        "        model = XGBRegressor()\n",
        "        model.fit(re, target_train)\n",
        "        pred = model.predict(values_test[re.columns])\n",
        "        score = rmsle(target_test.tolist(), pred.tolist())\n",
        "        if rmsl > score:\n",
        "            rmsl = score\n",
        "            n_opt_i = i\n",
        "            n_opt_j = j\n",
        "        print(rmsl, n_opt_i, n_opt_j)    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u4v0XD3IuJS9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import explained_variance_score as evs\n",
        "evs()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PFqWjmCrwyo6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import RFECV as rfe\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "\n",
        "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
        "selector = rfe(XGBRegressor(), cv=cv, scoring = 'mean_squared_log_error')\n",
        "values_train = selector.fit_transform(values_train, target_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RQije3dtygiH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.cross_validation import train_test_split as tts\n",
        "\n",
        "values, target = get_data(train)\n",
        "values_train, values_test, target_train, target_test = tts(values, target, test_size=0.2)\n",
        "#values_train = values_train.fillna(0)\n",
        "\n",
        "#corr = np.zeros([len(values_train.columns),2])\n",
        "    \n",
        "#for i in range(len(values_train.columns)):\n",
        "#    corr[i,:] = np.array([i, dcor.u_distance_correlation_sqr(values_train.iloc[:,i], target_train)])\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5GbDZ0cbJSkr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "a = np.where(corr[:,1] == max(corr[:,1]))\n",
        "kmeans = KMeans(values_train['target'])\n",
        "values_train[a]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dZoRNHGMq9PK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "thresh = max(np.nanpercentile(corr[:,1], 10), ((max(corr[:,1])-min(corr[:,1]))*0.1)+min(corr[:,1]))\n",
        "corr_new = np.array([i for i in corr if i[1] >= thresh])\n",
        "values_train_new = values_train.iloc[:,corr_new[:,0]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1dF1NT8Q2oKf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "thresh = max(np.nanpercentile(corr[:,1], 10), ((max(corr[:,1])-min(corr[:,1]))*0.1)+min(corr[:,1]))\n",
        "corr_new = np.array([i for i in corr if i[1] >= thresh])\n",
        "corr_new.shape\n",
        "\n",
        "kmeans = km(n_clusters = 8).fit(corr_new[:,1].reshape(-1,1))\n",
        "labels = kmeans.labels_\n",
        "\n",
        "x=[]\n",
        "for l in range(8):\n",
        "    x.append([i for i,d in enumerate(labels) if d==l])\n",
        "\n",
        "features_chosen = np.zeros(8)\n",
        "features_chosen = features_chosen-1\n",
        "maxx = np.zeros(8)\n",
        "for l in range(len(labels)):\n",
        "    a = abs(dcor.u_distance_correlation_sqr(values.iloc[:,x[labels[l]]], values.iloc[:,int(corr[l,0])]))\n",
        "    if a >= maxx[labels[l]]:\n",
        "        maxx[labels[l]] = a\n",
        "        features_chosen[labels[l]] = corr[l,0]\n",
        "\n",
        "features_chosen = np.array([int(i) for i in features_chosen if i>=0])\n",
        "features_chosen = values_train.columns[features_chosen]\n",
        "features_eliminated = choose_eliminated(values_train, features_chosen)   \n",
        "re = values_train.drop(features_eliminated, axis = 1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I_naM1QESLcz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "re_new = PolynomialFeatures(5).fit_transform(re)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WH33QnN9jyMX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "corr_re_new = np.zeros([re_new.shape[1],2])\n",
        "    \n",
        "for i in range(re_new.shape[1]):\n",
        "    corr_re_new[i,:] = np.array([i, dcor.u_distance_correlation_sqr(re_new[:,i], target_train)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0GoIQkhpltpr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "thresh = max(np.nanpercentile(corr_re_new[:,1], 3), ((max(corr_re_new[:,1])-min(corr_re_new[:,1]))*0.03)+min(corr_re_new[:,1]))\n",
        "corr_re_new_new = np.array([i for i in corr_re_new if i[1] >= thresh])\n",
        "corr_re_new_new.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JHAe--Qe4QnQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import math\n",
        "            \n",
        "model = XGBRegressor()\n",
        "target_train_new = target_train.apply(lambda x: math.log(x))\n",
        "model.fit(values_train_new.iloc[:,np.array([int(i) for i in corr_new[:,0]])], target_train_new)\n",
        "pred = model.predict(values_test[values_train_new.columns])[:,np.array([int(i) for i in corr_new[:,0]])]\n",
        "pred = [math.pow(math.e,i) for i in pred]\n",
        "score = rmsle(target_test.tolist(), pred)\n",
        "print(score)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q1QRuIcmAz2-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pred = model.predict(test[re.columns])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UAncqzoaCK7E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pred = [math.pow(math.e,i) for i in pred]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wtIzuS2UCQcw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "submission = pd.DataFrame(columns=['target'])\n",
        "\n",
        "submission['target'] = np.array(pred)\n",
        "submission.index = test['ID']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iZbJXWxfEFsa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "submission.to_csv('submission.csv')\n",
        "files.download('submission.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jw28sObcP372",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(score)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qlwt_p3K1mnC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#import math\n",
        "#from lightgbm import LGBMRegressor as gbm\n",
        "#from xgboost import XGBRegressor as xgb\n",
        "#from sklearn.preprocessing import StandardScaler as ss\n",
        "#from sklearn.cross_validation import train_test_split as tts\n",
        "#from sklearn.linear_model import ElasticNet as el\n",
        "from catboost import CatBoostRegressor as cbr\n",
        "\n",
        "score = 0\n",
        "for t in range(50):\n",
        "    values, target = get_data(train)\n",
        "    values = values[bins]\n",
        "    values_train, values_test, target_train, target_test = tts(values, target, test_size=0.2)\n",
        "    values_train = assign_zero(values_train)\n",
        "\n",
        "    #model = gbm()\n",
        "    #ress = target_train.apply(lambda x: math.log(x))\n",
        "    #model.fit(values_train, ress)\n",
        "    #pred1 = model.predict(values_test[values_train.columns])\n",
        "    #pred1 = [np.exp(i) for i in pred1]\n",
        "\n",
        "    #model = xgb()\n",
        "    #ress = target_train.apply(lambda x: math.log(x))\n",
        "    #model.fit(values_train, ress)\n",
        "    #pred2 = model.predict(values_test[values_train.columns])\n",
        "    #pred2 = [np.exp(i) for i in pred2]\n",
        "    \n",
        "    model = cbr()\n",
        "    ress = target_train.apply(lambda x: math.log1p(x))\n",
        "    model.fit(values_train, ress)\n",
        "    pred4 = model.predict(values_test[values_train.columns])\n",
        "    pred4 = [np.expm1(i) for i in pred4]\n",
        "    \n",
        "    #model = el(no)\n",
        "    #ress = target_train.apply(lambda x: math.log(x))\n",
        "    #model.fit(values_train, ress)\n",
        "    #pred3 = model.predict(values_test[values_train.columns])\n",
        "    #pred3 = [np.exp(i) for i in pred3]\n",
        "\n",
        "    #pred = []\n",
        "    #for i in range(len(pred1)):\n",
        "    #    pred.append(pred1[i]*0.5+pred2[i]*0.5)\n",
        "\n",
        "    score += rmsle(target_test.tolist(),pred4)\n",
        "\n",
        "print(score/50)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K_BMAIVwBq-A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.fit(values[values_train.columns], ress)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z5LuNOGZPj4k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rWizeIZ6LSH8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.cross_validation import train_test_split as tts\n",
        "import math\n",
        "from xgboost import XGBRegressor as xgb\n",
        "\n",
        "\n",
        "minscore = 10\n",
        "opteps = 0\n",
        "for i in range(300):\n",
        "    eps = 0.25 + i*0.001\n",
        "    values, target = get_data(train)\n",
        "    values = values.fillna(0)\n",
        "    col = values.columns\n",
        "    values = reduct_sparse_rp(values, eps)\n",
        "    values = pd.DataFrame(values)\n",
        "    values.columns = col[:values.shape[1]]\n",
        "    values_train, values_test, target_train, target_test = tts(values, target, test_size=0.2)\n",
        "\n",
        "    ress = target_train.apply(lambda x: math.log(x))\n",
        "\n",
        "    model = xgb()\n",
        "    model.fit(values_train, ress)\n",
        "    pred = model.predict(values_test)\n",
        "    pred = [math.e**i for i in pred]\n",
        "    score = rmsle(target_test.tolist(),pred)\n",
        "    if score < minscore:\n",
        "        minscore = score\n",
        "        opteps = eps"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NSl5kt8EL-NY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(test.iloc[:,1:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U4l5-Kved1dI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.cross_validation import train_test_split as tts\n",
        "import math\n",
        "from xgboost import XGBRegressor as xgb\n",
        "\n",
        "values, target = get_data(train)\n",
        "values_ = values.fillna(0)\n",
        "test_ = test\n",
        "test_ = test_.fillna(0)\n",
        "col = values.columns\n",
        "len_val = values.shape[0]\n",
        "temp = pd.concat([values_, test_.iloc[:,1:]], axis=0)\n",
        "\n",
        "minscore = 10\n",
        "opteps = 0\n",
        "for i in range(50):\n",
        "    eps = 0.25 + i*0.006\n",
        "    temp_ = reduct_sparse_rp(temp, eps)\n",
        "    temp_ = pd.DataFrame(temp_)\n",
        "    values_ = temp_.iloc[:len_val,:]\n",
        "    test_ = temp_.iloc[len_val:,:]\n",
        "    me=0\n",
        "    for t in range(5):\n",
        "        values_train, values_test, target_train, target_test = tts(values_, target, test_size=0.2)\n",
        "        ress = target_train.apply(lambda x: math.log(x))\n",
        "        model = xgb()\n",
        "        model.fit(values_train, ress)\n",
        "        pred = model.predict(values_test)\n",
        "        pred = [math.e**i for i in pred]\n",
        "        for i in range(len(pred)):\n",
        "            a = values_test.iloc[i,:]\n",
        "            for j in range(values_test.shape[1]):\n",
        "                if (pred[i] >= 0.9*values_test.iloc[i,j]) & (pred[i] <= 1.1*values_test.iloc[i,j]):\n",
        "                    pred[i] = values_test.iloc[i,j]\n",
        "                    j = values_test.shape[1]-1\n",
        "        score = rmsle(target_test.tolist(), pred)\n",
        "        me += score\n",
        "    if me < minscore*5:\n",
        "        minscore = me/5\n",
        "        opteps = eps\n",
        "         \n",
        "    print(me,eps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jKzcUlta7-S2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.cross_validation import train_test_split as tts\n",
        "import math\n",
        "from xgboost import XGBRegressor as xgb\n",
        "\n",
        "values, target = get_data(train)\n",
        "values_ = values.fillna(0)\n",
        "test_ = test\n",
        "test_ = test_.fillna(0)\n",
        "col = values.columns\n",
        "len_val = values.shape[0]\n",
        "temp = pd.concat([values_, test_.iloc[:,1:]], axis=0)\n",
        "\n",
        "temp_ = reduct_sparse_rp(temp, 0.256)\n",
        "temp_ = pd.DataFrame(temp_)\n",
        "values_ = temp_.iloc[:len_val,:]\n",
        "test_ = temp_.iloc[len_val:,:]\n",
        "ress = target.apply(lambda x: math.log(x))\n",
        "model = xgb()\n",
        "model.fit(values_, ress)\n",
        "pred = model.predict(test_)\n",
        "pred = [math.e**i for i in pred]\n",
        "\n",
        "for i in range(len(pred)):\n",
        "    a = test_.iloc[i,:]\n",
        "    for j in range(test_.shape[1]):\n",
        "       if (pred[i] >= 0.95*test_.iloc[i,j]) & (pred[i] <= 1.05*test_.iloc[i,j]):\n",
        "          pred[i] = test_.iloc[i,j]\n",
        "          j = test_.shape[1]-1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tQfMLZgsuHIH",
        "colab_type": "code",
        "outputId": "455961ba-0f72-4175-c9fb-d7a8d704e3a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.428"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "wQBrzIwoqei0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.cross_validation import train_test_split as tts\n",
        "import math\n",
        "from lightgbm import LGBMRegressor as lgbm\n",
        "from sklearn.metrics import r2_score as r2\n",
        "\n",
        "values, target = get_data(train)\n",
        "values = values[a]\n",
        "values_train, values_test, target_train, target_test = tts(values, target, test_size=0.2)\n",
        "values_train = assign_zero(values_train)\n",
        "values_test = values_test[values_train.columns]\n",
        "\n",
        "corr = np.zeros([len(values_train.columns),2])\n",
        "    \n",
        "for i in range(len(values_train.columns)):\n",
        "    corr[i,:] = np.array([i, dcor.u_distance_correlation_sqr(values_train.iloc[:,i], target_train)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Lko9gSIfi5XT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#thresh = max(np.nanpercentile(corr_re_new[:,1], 3), ((max(corr_re_new[:,1])-min(corr_re_new[:,1]))*0.03)+min(corr_re_new[:,1]))\n",
        "thresh = 0.08\n",
        "corr = np.array([i for i in corr if i[1] >= thresh])\n",
        "print(corr.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_tAJPA0kiBPd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "feat = np.array([int(i) for i in corr[:,0]])\n",
        "classy = pd.DataFrame(columns = values_train.columns[feat])\n",
        "\n",
        "target_train_log = target_train.apply(lambda x: math.log(x))\n",
        "classy['target_train'] = (target_train_log - target_train_log.mean())/target_train_log.std()\n",
        "for i in range(len(classy['target_train'])):\n",
        "    if classy.loc['target_train',i]>3:\n",
        "        p3.append()\n",
        "\n",
        "for i in classy:\n",
        "    classy[i] = (values_train[i] - values_train[i].mean())/values_train[i].std()\n",
        "    \n",
        "classy_test = pd.DataFrame(columns = values_train.columns[feat])\n",
        "for i in classy_test:\n",
        "    classy_test[i] = (values_test[i] - values_test[i].mean())/values_test[i].std()\n",
        "\n",
        "\n",
        "#kmeans = km(n_clusters = n_features).fit(target_train.apply(lambda x: math.log(x)).reshape(-1,1))  #n_features should be determined based on a performance metrics\n",
        "#labels = kmeans.labels_\n",
        "\n",
        "target_test_log = target_test.apply(lambda x: math.log(x))\n",
        "target_test_log = (target_test_log - target_test_log.mean())/target_test_log.std() \n",
        "\n",
        "model = lgbm()\n",
        "model.fit(classy.drop(['target_train'], axis=1), classy['target_train'])\n",
        "pred = model.predict(classy_test)\n",
        "#pred = [math.e**i for i in pred]\n",
        "score = r2(target_test_log,pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O4CbZqlEg7aZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(values_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o6G7NBXR46or",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "values_test['f190486d6']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o5P649yIf6Vm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "\n",
        "reds = pd.read_csv(io.StringIO(uploaded['dimensionality reduction.csv'].decode('utf-8')))\n",
        "reds.index = reds.iloc[:,0]\n",
        "\n",
        "a = [i for i in reds.index if (reds.loc[i,'reductedby_low_var']==0) & (reds.loc[i,'reductedby_xgbregressor']==0.0) & (reds.loc[i,'reductedby_psi']==0.0)]\n",
        "a = a[:-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5eXNHqPuXx4_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#values_train_new.iloc[4,:]=np.array([int(i) for i in values_train.iloc[4,:]==target_train.iloc[4]])\n",
        "#print(sum(values_train_new.iloc[4,:]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YurKFdv6HqBS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.cross_validation import train_test_split as tts\n",
        "from sklearn.linear_model import LogisticRegression as lr\n",
        "\n",
        "values, target = get_data(train)\n",
        "values_train, values_test, target_train, target_test = tts(values, target, test_size=0.2)\n",
        "values_train = assign_zero(values_train)\n",
        "values_test = values_test[values_train.columns]\n",
        "\n",
        "dec = []\n",
        "nm = []\n",
        "values_train_new = values_train\n",
        "for k in range(values_train.shape[0]):\n",
        "    values_train_new.iloc[k,:] = np.array([int(i) for i in values_train.iloc[k,:]==target_train.iloc[k]])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mBnv2CsWHzOy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "perm = values_train_new\n",
        "temp = values_train_new\n",
        "freq = values_train_new.sum(axis=1)\n",
        "while freq.sum()>0:\n",
        "    inn = freq[freq==max(freq)].index[0]\n",
        "    tar_train = perm.iloc[:,inn]\n",
        "    part_train = values_train.drop(values_train.columns[inn], axis=1)\n",
        "    part_test = values_test.drop(values_train.columns[inn], axis=1)\n",
        "    temp = temp.drop((temp.iloc[:,inn]==1).index, axis=0)\n",
        "    freq = temp.sum(axis=1)\n",
        "    model = lr()\n",
        "    model.fit(part_train, tar_train)\n",
        "    dec.append(model.predict_proba(part_test))\n",
        "    nm.append(values_train.columns[inn])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xGemYb5r07Ag",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#from sklearn.feature_selection import RFECV as rfe\n",
        "#from sklearn.model_selection import ShuffleSplit\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from sklearn.cross_validation import train_test_split as tts\n",
        "import math\n",
        "from keras.layers import Dense, LSTM, Dropout\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "#from sklearn.ensemble import RandomForestRegressor as rf\n",
        "\n",
        "#train = pd.read_csv(r'C:\\Users\\StjYahyaG\\reduction\\train.csv')\n",
        "values = train.iloc[:,2:]\n",
        "col = values.columns\n",
        "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "values = pd.DataFrame(scaler.fit_transform(values))\n",
        "values.columns = col\n",
        "target = train.iloc[:,1]\n",
        "scaler2 = MinMaxScaler(feature_range=(-1, 1))\n",
        "target = scaler2.fit_transform(target.values.reshape(-1, 1))\n",
        "\n",
        "#cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
        "#selector = rfe(rf(), cv=cv, scoring = scorer(msle(),values_train, target_train))\n",
        "#values = selector.fit_transform(values, target)\n",
        "#from tsfresh import extract_features\n",
        "\n",
        "#from lightgbm import LGBMRegressor as lgbm\n",
        "\n",
        "\n",
        "#col = ['f190486d6', '58e2e02e6', 'eeb9cd3aa', '9fd594eec', '6eef030c1', '15ace8c9f', 'fb0f5dbfe', '58e056e12', '20aa07010', '024c577b9', 'd6bb78916', 'b43a7cfd5', '58232a6fb', '1702b5bf0', '324921c7b', '62e59a501', '2ec5b290f', '241f0f867', 'fb49e4212', '66ace2992', 'f74e8f13d', '5c6487af1', '963a49cdc', '26fc93eb7' ,'1931ccfdd', '703885424' ,'70feb1494', '491b9ee45', '23310aa6f', 'e176a204a', '6619d81fc', '1db387535']\n",
        "values_train, values_test, target_train, target_test = tts(values, target, test_size=0.2)\n",
        "values_train = values_train.dropna(thresh=int(values_train.shape[1]*0.95))\n",
        "values_train = values_train.fillna(0)\n",
        "values_train = values_train.drop([i for i in values_train.columns if values_train[i].sum()==0], axis = 1)\n",
        "#values_train_new = pd.DataFrame(np.array(values_train).reshape(values_train.shape[0], 1, values_train.shape[1]))\n",
        "#values_train_new.columns = col\n",
        "values_test = values_test[values_train.columns]\n",
        "#new_features_train = extract_features(values_train['f190486d6'])\n",
        "#fin = pd.concat(values_train, new_features_train, axis=1)\n",
        "#new_features_test = extract_features(values_test['f190486d6'], column_id='bla')\n",
        "#values_test_fin = pd.concat(values_test, new_features_test, axis=1)\n",
        "\n",
        "#target_train_log = target_train.apply(lambda x: math.log(x))\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_shape=(values_train.shape[1],), activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1))\n",
        "model.compile(loss='mean_squared_logarithmic_error', optimizer='Adam', metrics=['accuracy'])\n",
        "model_info = model.fit(values_train, target_train, epochs = 100, validation_split=0.2)\n",
        "plot_model_history(model_info)\n",
        "\n",
        "pred = model.predict(values_test)\n",
        "pred = scaler2.inverse_transform(pred)\n",
        "#pred = [math.e**i for i in pred]\n",
        "\n",
        "score = rmsle(target_test.tolist(), pred.tolist())\n",
        "print(score)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eGE5HSK-JH3p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plot_model_history(model_info)\n",
        "\n",
        "pred = model.predict(values_test)\n",
        "pred = scaler2.inverse_transform(pred)\n",
        "#pred = [math.e**i for i in pred]\n",
        "\n",
        "score = rmsle(target_test.tolist(), pred.tolist())\n",
        "print(score)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JO7JSHExnini",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.array(values_train).reshape(values_train.shape[0], 1, values_train.shape[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LOvV3Idbms-n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_model_history(model_history):\n",
        "    fig, axs = plt.subplots(1,2,figsize=(15,5))\n",
        "    # summarize history for accuracy\n",
        "    axs[0].plot(range(1,len(model_history.history['acc'])+1),model_history.history['acc'])\n",
        "    axs[0].plot(range(1,len(model_history.history['val_acc'])+1),model_history.history['val_acc'])\n",
        "    axs[0].set_title('Model Accuracy')\n",
        "    axs[0].set_ylabel('Accuracy')\n",
        "    axs[0].set_xlabel('Epoch')\n",
        "    axs[0].set_xticks(np.arange(1,len(model_history.history['acc'])+1),len(model_history.history['acc'])/10)\n",
        "    axs[0].legend(['train', 'val'], loc='best')\n",
        "    # summarize history for loss\n",
        "    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])\n",
        "    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])\n",
        "    axs[1].set_title('Model Loss')\n",
        "    axs[1].set_ylabel('Loss')\n",
        "    axs[1].set_xlabel('Epoch')\n",
        "    axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])/10)\n",
        "    axs[1].legend(['train', 'val'], loc='best')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}